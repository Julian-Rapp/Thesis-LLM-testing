@misc{bhatiaUnitTestGeneration2024,
  title = {Unit {{Test Generation}} Using {{Generative AI}} : {{A Comparative Performance Analysis}} of {{Autogeneration Tools}}},
  shorttitle = {Unit {{Test Generation}} Using {{Generative AI}}},
  author = {Bhatia, Shreya and Gandhi, Tarushi and Kumar, Dhruv and Jalote, Pankaj},
  year = {2024},
  month = feb,
  number = {arXiv:2312.10622},
  eprint = {2312.10622},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10622},
  urldate = {2025-09-08},
  abstract = {Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT's performance, achieving a much higher coverage.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {C:\Users\Julian\Zotero\storage\U6N669LC\Bhatia et al. - 2024 - Unit Test Generation using Generative AI  A Comparative Performance Analysis of Autogeneration Tool.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2025-09-08},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -- something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\Julian\Zotero\storage\HVYAZAUG\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@misc{chenChatUniTestFrameworkLLMBased2024,
  title = {{{ChatUniTest}}: {{A Framework}} for {{LLM-Based Test Generation}}},
  shorttitle = {{{ChatUniTest}}},
  author = {Chen, Yinghao and Hu, Zehao and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei},
  year = {2024},
  month = may,
  number = {arXiv:2305.04764},
  eprint = {2305.04764},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.04764},
  urldate = {2025-09-08},
  abstract = {Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain. ChatUniTest is available at https: //github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\Julian\Zotero\storage\H93JQV6H\Chen et al. - 2024 - ChatUniTest A Framework for LLM-Based Test Generation.pdf}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {2107.03374},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.03374},
  urldate = {2025-09-08},
  abstract = {We introduce Codex, a GPT language model finetuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\Julian\Zotero\storage\GV3NKWT3\Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf}
}

@misc{dakhelEffectiveTestGeneration2023,
  title = {Effective {{Test Generation Using Pre-trained Large Language Models}} and {{Mutation Testing}}},
  author = {Dakhel, Arghavan Moradi and Nikanjam, Amin and Majdinasab, Vahid and Khomh, Foutse and Desmarais, Michel C.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.16557},
  eprint = {2308.16557},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.16557},
  urldate = {2025-09-08},
  abstract = {Objective: To improve over this limitation, in this paper, we introduce MuTAP (Mutation Test case generation using Augmented Prompt) for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing. Method: Our goal is achieved by augmenting prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs. MuTAP is capable of generating effective test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate their performance on different benchmarks. Results: Our results show that our proposed method is able to detect up to 28\% more faulty humanwritten code snippets. Among these, 17\% remained undetected by both the current state-of-the-art fully-automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning approaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57\% on synthetic buggy code, outperforming all other approaches in our evaluation. Conclusion: Our findings suggest that although LLMs can serve as a useful tool to generate test cases, they require specific post-processing steps to enhance the effectiveness of the generated test cases which may suffer from syntactic or functional errors and may be ineffective in detecting certain types of bugs and testing corner cases in PUTs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\Julian\Zotero\storage\SQ635DMG\Dakhel et al. - 2023 - Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing.pdf}
}

@article{liEvaluatingLargeLanguage2025,
  title = {Evaluating Large Language Models for Software Testing},
  author = {Li, Yihao and Liu, Pan and Wang, Haiyang and Chu, Jie and Wong, W. Eric},
  year = {2025},
  month = apr,
  journal = {Computer Standards \& Interfaces},
  volume = {93},
  pages = {103942},
  issn = {09205489},
  doi = {10.1016/j.csi.2024.103942},
  urldate = {2025-09-08},
  abstract = {Large language models (LLMs) have demonstrated significant prowess in code analysis and natural language processing, making them highly valuable for software testing. This paper conducts a comprehensive evaluation of LLMs applied to software testing, with a particular emphasis on test case generation, error tracing, and bug localization across twelve open-source projects. The advantages and limitations, as well as recommendations associated with utilizing LLMs for these tasks, are delineated. Furthermore, we delve into the phenomenon of hallucination in LLMs, examining its impact on software testing processes and presenting solutions to mitigate its effects. The findings of this work contribute to a deeper understanding of integrating LLMs into software testing, providing insights that pave the way for enhanced effectiveness in the field.},
  langid = {english},
  file = {C:\Users\Julian\Zotero\storage\BU5GUKUN\Li et al. - 2025 - Evaluating large language models for software testing.pdf}
}

@article{PDFAugmentingAutomatically2025,
  title = {({{PDF}}) {{Augmenting Automatically Generated Unit-Test Suites}} with {{Regression Oracle Checking}}},
  year = {2025},
  month = aug,
  journal = {ResearchGate},
  doi = {10.1007/11785477_23},
  urldate = {2025-09-09},
  abstract = {PDF {\textbar} A test case consists of two parts: a test input to exercise the program under test and a test oracle to check the correctness of the test... {\textbar} Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  file = {C:\Users\Julian\Zotero\storage\BT9TMZHR\221496449_Augmenting_Automatically_Generated_Unit-Test_Suites_with_Regression_Oracle_Checking.html}
}

@misc{raoCATLMTrainingLanguage2023,
  title = {{{CAT-LM}}: {{Training Language Models}} on {{Aligned Code And Tests}}},
  shorttitle = {{{CAT-LM}}},
  author = {Rao, Nikitha and Jain, Kush and Alon, Uri and Goues, Claire Le and Hellendoorn, Vincent J.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01602},
  eprint = {2310.01602},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01602},
  urldate = {2025-09-08},
  abstract = {Testing is an integral but often neglected part of the software development process. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the codeunder-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CATLM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than typical code generation models, to ensure that the code context is available to the model when generating test code. We analyze its usefulness for realistic applications, showing that sampling with filtering (e.g., by compilability, coverage) allows it to efficiently produce tests that achieve coverage similar to ones written by developers while resembling their writing style. By utilizing the code context, CAT-LM generates more valid tests than even much larger language models trained with more data (CodeGen 16B and StarCoder) and substantially outperforms a recent test-specific model (TeCo) at test completion. Overall, our work highlights the importance of incorporating software-specific insights when training language models for code and paves the way to more powerful automated test generation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {C:\Users\Julian\Zotero\storage\AXFPZ272\Rao et al. - 2023 - CAT-LM Training Language Models on Aligned Code And Tests.pdf}
}

@misc{schaferEmpiricalEvaluationUsing2023,
  title = {An {{Empirical Evaluation}} of {{Using Large Language Models}} for {{Automated Unit Test Generation}}},
  author = {Sch{\"a}fer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
  year = {2023},
  month = dec,
  number = {arXiv:2302.06527},
  eprint = {2302.06527},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.06527},
  urldate = {2025-09-08},
  abstract = {Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in TESTPILOT, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate TESTPILOT using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2\% and branch coverage of 52.8\%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3\% statement coverage and 25.6\% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8\% of TESTPILOT's generated tests have {$\leq$} 50\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TESTPILOT with two additional LLMs, OpenAI's older code-cushman-002 LLM and StarCoder , an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2\% median statement coverage), and somewhat worse results with the latter (54.0\% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {C:\Users\Julian\Zotero\storage\3T9RZ69B\Schäfer et al. - 2023 - An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation.pdf}
}

@misc{tangChatGPTVsSBST2023,
  title = {{{ChatGPT}} vs {{SBST}}: {{A Comparative Assessment}} of {{Unit Test Suite Generation}}},
  shorttitle = {{{ChatGPT}} vs {{SBST}}},
  author = {Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.00588},
  eprint = {2307.00588},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.00588},
  urldate = {2025-09-08},
  abstract = {Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\Julian\Zotero\storage\TZB73QND\Tang et al. - 2023 - ChatGPT vs SBST A Comparative Assessment of Unit Test Suite Generation.pdf}
}

@misc{wangSoftwareTestingLarge2024,
  title = {Software {{Testing}} with {{Large Language Models}}: {{Survey}}, {{Landscape}}, and {{Vision}}},
  shorttitle = {Software {{Testing}} with {{Large Language Models}}},
  author = {Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing},
  year = {2024},
  month = mar,
  number = {arXiv:2307.07221},
  eprint = {2307.07221},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.07221},
  urldate = {2025-09-08},
  abstract = {Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\Julian\Zotero\storage\SJTZ27EZ\Wang et al. - 2024 - Software Testing with Large Language Models Survey, Landscape, and Vision.pdf}
}

@article{yangTestLoterLogicdrivenFramework2025,
  title = {{{TestLoter}}: {{A}} Logic-Driven Framework for Automated Unit Test Generation and Error Repair Using Large Language Models},
  shorttitle = {{{TestLoter}}},
  author = {Yang, Ruofan and Xu, Xianghua and Wang, Ran},
  year = {2025},
  month = sep,
  journal = {Journal of Computer Languages},
  volume = {84},
  pages = {101348},
  issn = {2590-1184},
  doi = {10.1016/j.cola.2025.101348},
  urldate = {2025-09-08},
  abstract = {Automated unit test generation is a critical technique for improving software quality and development efficiency. However, traditional methods often produce test cases with poor business consistency, while large language model based approaches face two major challenges: a high error rate in generated tests and insufficient code coverage. To address these issues, this paper proposes TestLoter, a logic-driven test generation framework. The core contributions of TestLoter are twofold. First, by integrating the structured analysis capabilities of white-box testing with the functional validation characteristics of black-box testing, we design a logic-driven test generation chain-of-thought that enables deep semantic analysis of code. Second, we establish a hierarchical repair mechanism to systematically correct errors in generated test cases, significantly enhancing the correctness of the test code. Experimental results on nine open-source projects covering various domains, such as data processing and utility libraries, demonstrate that TestLoter achieves 83.6\% line coverage and 78\% branch coverage. Our approach outperforms both LLM-based methods and traditional search-based software testing techniques in terms of coverage, while also reducing the number of errors in the generated unit test code.},
  keywords = {Large language models,Program repair,Software testing,Unit test generation},
  file = {C:\Users\Julian\Zotero\storage\KPRBT4VL\S2590118425000346.html}
}

@misc{yuanNoMoreManual2024,
  title = {No {{More Manual Tests}}? {{Evaluating}} and {{Improving ChatGPT}} for {{Unit Test Generation}}},
  shorttitle = {No {{More Manual Tests}}?},
  author = {Yuan, Zhiqiang and Lou, Yiling and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin},
  year = {2024},
  month = may,
  number = {arXiv:2305.04207},
  eprint = {2305.04207},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.04207},
  urldate = {2025-09-08},
  abstract = {Unit testing is essential in detecting bugs in functionally-discrete program units. Manually writing high-quality unit tests is time-consuming and laborious. Although traditional techniques can generate tests with reasonable coverage, they exhibit low readability and cannot be directly adopted by developers. Recent work has shown the large potential of large language models (LLMs) in unit test generation, which can generate more human-like and meaningful test code. ChatGPT, the latest LLM incorporating instruction tuning and reinforcement learning, has performed well in various domains. However, It remains unclear how effective ChatGPT is in unit test generation. In this work, we perform the first empirical study to evaluate ChatGPT's capability of unit test generation. Specifically, we conduct a quantitative analysis and a user study to systematically investigate the quality of its generated tests regarding the correctness, sufficiency, readability, and usability. The tests generated by ChatGPT still suffer from correctness issues, including diverse compilation errors and execution failures. Still, the passing tests generated by ChatGPT resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we propose ChatTESTER, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTESTER incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of ChatTESTER by generating 34.3\% more compilable tests and 18.7\% more tests with correct assertions than the default ChatGPT.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {C:\Users\Julian\Zotero\storage\4HV3DBHK\Yuan et al. - 2024 - No More Manual Tests Evaluating and Improving ChatGPT for Unit Test Generation.pdf}
}
